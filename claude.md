# Thompson Sampling for Multi-Armed Bandit Problems

(generated by Claude 3.7 Sonnet Thinking)

Thompson Sampling is an elegant Bayesian approach to the multi-armed bandit problem. I'll implement a comprehensive solution in Python that can handle different types of reward distributions and includes utilities for simulation and evaluation.

```python name=thompson_sampling.py
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Tuple, Optional, Union, Dict, Any
import pandas as pd


class ThompsonSampling:
    """
    Thompson Sampling implementation for the Multi-Armed Bandit problem.
    
    This implementation supports Beta priors for Bernoulli bandits (0/1 rewards)
    and Gaussian priors for bandits with normally distributed rewards.
    """
    
    def __init__(self, n_arms: int, prior_type: str = 'beta', 
                 alpha: Union[float, List[float]] = 1.0, 
                 beta: Union[float, List[float]] = 1.0,
                 mu: Union[float, List[float]] = 0.0,
                 sigma: Union[float, List[float]] = 1.0):
        """
        Initialize the Thompson Sampling algorithm.
        
        Args:
            n_arms: Number of arms (actions)
            prior_type: Type of prior distribution ('beta' for Bernoulli rewards, 
                        'gaussian' for normally distributed rewards)
            alpha: Prior alpha parameter(s) for Beta distribution
            beta: Prior beta parameter(s) for Beta distribution
            mu: Prior mean parameter(s) for Gaussian distribution
            sigma: Prior standard deviation parameter(s) for Gaussian distribution
        """
        self.n_arms = n_arms
        self.prior_type = prior_type.lower()
        
        if self.prior_type not in ['beta', 'gaussian']:
            raise ValueError("prior_type must be either 'beta' or 'gaussian'")
        
        # Initialize parameters based on prior type
        if self.prior_type == 'beta':
            self.alpha = np.ones(n_arms) * alpha if isinstance(alpha, (int, float)) else np.array(alpha)
            self.beta = np.ones(n_arms) * beta if isinstance(beta, (int, float)) else np.array(beta)
        else:  # gaussian
            self.mu = np.ones(n_arms) * mu if isinstance(mu, (int, float)) else np.array(mu)
            self.sigma = np.ones(n_arms) * sigma if isinstance(sigma, (int, float)) else np.array(sigma)
            self.tau = np.zeros(n_arms)  # precision (1/variance)
            self.tau_mu = np.zeros(n_arms)  # precision * mean
        
        # Track statistics
        self.pulls = np.zeros(n_arms, dtype=int)
        self.rewards = np.zeros(n_arms)
        self.history = {
            'arm': [],
            'reward': [],
            'cumulative_reward': 0,
            'regret': 0
        }
        
    def select_arm(self) -> int:
        """
        Select an arm according to Thompson Sampling.
        
        Returns:
            The index of the selected arm.
        """
        if self.prior_type == 'beta':
            samples = np.random.beta(self.alpha, self.beta)
        else:  # gaussian
            # For arms that haven't been pulled, use prior
            precision = np.where(self.pulls > 0, 
                                1.0 / (self.sigma**2 / self.pulls), 
                                1.0 / self.sigma**2)
            mu_hat = np.where(self.pulls > 0, 
                             self.rewards / self.pulls, 
                             self.mu)
            # Sample from posterior
            samples = np.random.normal(mu_hat, 1.0 / np.sqrt(precision))
            
        return int(np.argmax(samples))
    
    def update(self, arm: int, reward: float) -> None:
        """
        Update the distribution for the selected arm based on the observed reward.
        
        Args:
            arm: The arm that was pulled
            reward: The observed reward
        """
        # Update statistics
        self.pulls[arm] += 1
        self.rewards[arm] += reward
        
        # Update distribution parameters
        if self.prior_type == 'beta':
            # For Bernoulli bandits (0/1 rewards)
            self.alpha[arm] += reward
            self.beta[arm] += (1 - reward)
        else:  # gaussian
            # For normally distributed rewards
            old_tau = 1.0 / (self.sigma[arm]**2) if self.sigma[arm] > 0 else 0
            old_tau_mu = old_tau * self.mu[arm]
            
            # Update precision and precision-weighted mean
            self.tau[arm] = old_tau + 1.0
            self.tau_mu[arm] = old_tau_mu + reward
            
            # Compute new mean and standard deviation
            self.mu[arm] = self.tau_mu[arm] / self.tau[arm]
            self.sigma[arm] = np.sqrt(1.0 / self.tau[arm])
        
        # Record history
        self.history['arm'].append(arm)
        self.history['reward'].append(reward)
        self.history['cumulative_reward'] += reward
    
    def get_expected_rewards(self) -> np.ndarray:
        """
        Get the current expected reward for each arm.
        
        Returns:
            Array of expected rewards
        """
        if self.prior_type == 'beta':
            return self.alpha / (self.alpha + self.beta)
        else:  # gaussian
            return self.mu
    
    def get_arm_stats(self) -> pd.DataFrame:
        """
        Return statistics for each arm.
        
        Returns:
            DataFrame containing statistics for each arm
        """
        stats = {
            'arm': list(range(self.n_arms)),
            'pulls': self.pulls,
            'total_reward': self.rewards,
            'expected_reward': self.get_expected_rewards()
        }
        if self.prior_type == 'beta':
            stats['alpha'] = self.alpha
            stats['beta'] = self.beta
        else:
            stats['mu'] = self.mu
            stats['sigma'] = self.sigma
        
        return pd.DataFrame(stats)


class BanditSimulator:
    """
    Simulator for multi-armed bandit problems.
    """
    
    def __init__(self, reward_functions: List[callable], optimal_arm: Optional[int] = None):
        """
        Initialize the bandit simulator.
        
        Args:
            reward_functions: List of functions that generate rewards for each arm
            optimal_arm: Index of the optimal arm (if known)
        """
        self.reward_functions = reward_functions
        self.n_arms = len(reward_functions)
        self.optimal_arm = optimal_arm
        
    def get_reward(self, arm: int) -> float:
        """
        Get a reward from pulling the specified arm.
        
        Args:
            arm: The arm to pull
            
        Returns:
            The reward from pulling the arm
        """
        if arm < 0 or arm >= self.n_arms:
            raise ValueError(f"Arm index {arm} out of range [0, {self.n_arms-1}]")
        
        return self.reward_functions[arm]()
    
    def run_simulation(self, algorithm: ThompsonSampling, n_steps: int) -> Dict[str, Any]:
        """
        Run a simulation of the bandit algorithm.
        
        Args:
            algorithm: The bandit algorithm to use
            n_steps: Number of steps to simulate
            
        Returns:
            Dictionary with simulation results
        """
        rewards = []
        chosen_arms = []
        regrets = []
        cumulative_regrets = []
        cumulative_reward = 0
        
        if self.optimal_arm is not None:
            optimal_rewards = []
        
        for _ in range(n_steps):
            # Select arm using the algorithm
            arm = algorithm.select_arm()
            chosen_arms.append(arm)
            
            # Get reward
            reward = self.get_reward(arm)
            rewards.append(reward)
            cumulative_reward += reward
            
            # Update algorithm with observed reward
            algorithm.update(arm, reward)
            
            # Calculate regret if optimal arm is known
            if self.optimal_arm is not None:
                optimal_reward = self.get_reward(self.optimal_arm)
                optimal_rewards.append(optimal_reward)
                regret = optimal_reward - reward
                regrets.append(regret)
                
                if len(cumulative_regrets) > 0:
                    cumulative_regrets.append(cumulative_regrets[-1] + regret)
                else:
                    cumulative_regrets.append(regret)
        
        results = {
            'rewards': rewards,
            'chosen_arms': chosen_arms,
            'cumulative_reward': cumulative_reward,
            'arm_stats': algorithm.get_arm_stats()
        }
        
        if self.optimal_arm is not None:
            results['regrets'] = regrets
            results['cumulative_regrets'] = cumulative_regrets
            results['optimal_rewards'] = optimal_rewards
            
        return results


def plot_simulation_results(results: Dict[str, Any], title: str = "Thompson Sampling Simulation"):
    """
    Plot the results of a bandit simulation.
    
    Args:
        results: Dictionary with simulation results
        title: Title for the plots
    """
    plt.figure(figsize=(15, 10))
    
    # Plot rewards over time
    plt.subplot(2, 2, 1)
    plt.plot(results['rewards'], alpha=0.5)
    plt.plot(pd.Series(results['rewards']).rolling(window=50).mean(), 'r-', linewidth=2)
    plt.title('Rewards over Time')
    plt.xlabel('Step')
    plt.ylabel('Reward')
    
    # Plot arm selection frequency
    plt.subplot(2, 2, 2)
    arm_counts = pd.Series(results['chosen_arms']).value_counts().sort_index()
    arm_counts.plot(kind='bar')
    plt.title('Arm Selection Frequency')
    plt.xlabel('Arm')
    plt.ylabel('Count')
    
    # Plot arm statistics
    plt.subplot(2, 2, 3)
    stats = results['arm_stats']
    plt.bar(stats['arm'], stats['expected_reward'])
    plt.title('Expected Rewards by Arm')
    plt.xlabel('Arm')
    plt.ylabel('Expected Reward')
    
    # Plot regret if available
    if 'cumulative_regrets' in results:
        plt.subplot(2, 2, 4)
        plt.plot(results['cumulative_regrets'])
        plt.title('Cumulative Regret')
        plt.xlabel('Step')
        plt.ylabel('Regret')
    
    plt.suptitle(title)
    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust for the super title
    plt.show()


# Example usage
if __name__ == "__main__":
    # Create a simulator with Bernoulli reward distributions
    np.random.seed(42)  # For reproducibility
    
    # Define reward probabilities for each arm
    probs = [0.1, 0.2, 0.3, 0.4, 0.8]
    
    # Create reward functions for each arm
    reward_functions = [lambda p=p: np.random.binomial(1, p) for p in probs]
    
    # Create the simulator
    simulator = BanditSimulator(reward_functions, optimal_arm=4)  # Arm 4 has highest probability
    
    # Create Thompson Sampling algorithm
    ts = ThompsonSampling(n_arms=len(probs), prior_type='beta')
    
    # Run simulation
    n_steps = 1000
    results = simulator.run_simulation(ts, n_steps)
    
    # Print summary
    print(f"Cumulative Reward: {results['cumulative_reward']}")
    print("\nArm Statistics:")
    print(results['arm_stats'])
    
    # Plot results
    plot_simulation_results(results, "Thompson Sampling with Bernoulli Bandits")
    
    # Example with Gaussian rewards
    print("\n\nExample with Gaussian rewards:")
    
    # Define mean and standard deviation for each arm
    means = [1.0, 2.0, 3.0, 4.0, 5.0]
    stds = [1.0, 1.0, 1.0, 1.0, 1.0]
    
    # Create reward functions for each arm
    reward_functions_gaussian = [lambda m=m, s=s: np.random.normal(m, s) for m, s in zip(means, stds)]
    
    # Create the simulator
    simulator_gaussian = BanditSimulator(reward_functions_gaussian, optimal_arm=4)
    
    # Create Thompson Sampling algorithm
    ts_gaussian = ThompsonSampling(n_arms=len(means), prior_type='gaussian')
    
    # Run simulation
    results_gaussian = simulator_gaussian.run_simulation(ts_gaussian, n_steps)
    
    # Print summary
    print(f"Cumulative Reward: {results_gaussian['cumulative_reward']}")
    print("\nArm Statistics:")
    print(results_gaussian['arm_stats'])
    
    # Plot results
    plot_simulation_results(results_gaussian, "Thompson Sampling with Gaussian Bandits")
```

```python name=thompson_sampling_simplified.py
import numpy as np
import matplotlib.pyplot as plt


class ThompsonSampling:
    """
    A simple implementation of Thompson Sampling for the multi-armed bandit problem.
    This implementation uses Beta distributions as priors for Bernoulli bandits.
    """
    
    def __init__(self, n_arms):
        """
        Initialize the Thompson Sampling algorithm.
        
        Args:
            n_arms: Number of arms (actions) in the bandit
        """
        self.n_arms = n_arms
        # Initialize alpha and beta parameters for the Beta distribution
        # Alpha represents the number of successes + 1
        # Beta represents the number of failures + 1
        self.alpha = np.ones(n_arms)  # Prior of 1 success for each arm
        self.beta = np.ones(n_arms)   # Prior of 1 failure for each arm
        
        # Keep track of total rewards and number of pulls per arm
        self.rewards = np.zeros(n_arms)
        self.pulls = np.zeros(n_arms, dtype=int)
        
        # History of actions and rewards
        self.history = {
            'arms': [],
            'rewards': [],
            'cumulative_reward': 0
        }
    
    def select_arm(self):
        """
        Select an arm according to Thompson Sampling.
        
        Returns:
            The index of the selected arm
        """
        # Sample a value from each arm's Beta distribution
        samples = np.random.beta(self.alpha, self.beta)
        
        # Choose the arm with the highest sample
        return np.argmax(samples)
    
    def update(self, arm, reward):
        """
        Update the distribution for the selected arm based on the observed reward.
        
        Args:
            arm: The arm that was pulled
            reward: The observed reward (0 or 1 for Bernoulli bandits)
        """
        # Update statistics
        self.pulls[arm] += 1
        self.rewards[arm] += reward
        
        # Update Beta distribution parameters
        self.alpha[arm] += reward
        self.beta[arm] += (1 - reward)
        
        # Update history
        self.history['arms'].append(arm)
        self.history['rewards'].append(reward)
        self.history['cumulative_reward'] += reward
    
    def get_expected_rewards(self):
        """
        Get the current expected reward for each arm.
        
        Returns:
            Array of expected rewards
        """
        return self.alpha / (self.alpha + self.beta)


def run_bandit_simulation(n_arms, true_probs, n_steps):
    """
    Run a simulation of Thompson Sampling on a multi-armed bandit problem.
    
    Args:
        n_arms: Number of arms
        true_probs: True probabilities of success for each arm
        n_steps: Number of steps to simulate
        
    Returns:
        ThompsonSampling object with simulation results
    """
    # Initialize Thompson Sampling
    ts = ThompsonSampling(n_arms)
    
    # Run simulation
    for _ in range(n_steps):
        # Select arm
        arm = ts.select_arm()
        
        # Generate reward (Bernoulli trial with true probability)
        reward = np.random.binomial(1, true_probs[arm])
        
        # Update algorithm
        ts.update(arm, reward)
    
    return ts


def plot_results(ts, true_probs):
    """
    Plot the results of a Thompson Sampling simulation.
    
    Args:
        ts: ThompsonSampling object after simulation
        true_probs: True probabilities of success for each arm
    """
    # Create a figure with subplots
    fig, axs = plt.subplots(2, 2, figsize=(12, 8))
    
    # Plot 1: Arm selection frequency
    arm_counts = np.bincount(ts.history['arms'], minlength=ts.n_arms)
    axs[0, 0].bar(range(ts.n_arms), arm_counts)
    axs[0, 0].set_title('Arm Selection Frequency')
    axs[0, 0].set_xlabel('Arm')
    axs[0, 0].set_ylabel('Count')
    
    # Plot 2: Expected vs True Probabilities
    axs[0, 1].bar(range(ts.n_arms), true_probs, alpha=0.5, label='True Probability')
    axs[0, 1].bar(range(ts.n_arms), ts.get_expected_rewards(), alpha=0.5, label='Estimated Probability')
    axs[0, 1].set_title('Expected vs True Probabilities')
    axs[0, 1].set_xlabel('Arm')
    axs[0, 1].set_ylabel('Probability')
    axs[0, 1].legend()
    
    # Plot 3: Rewards over time
    rewards_series = np.array(ts.history['rewards'])
    axs[1, 0].plot(rewards_series, 'b-', alpha=0.3)
    # Add a rolling average
    window_size = min(100, len(rewards_series) // 10)
    if window_size > 0:
        rolling_mean = np.convolve(rewards_series, np.ones(window_size)/window_size, mode='valid')
        axs[1, 0].plot(range(window_size-1, len(rewards_series)), rolling_mean, 'r-')
    axs[1, 0].set_title('Rewards over Time')
    axs[1, 0].set_xlabel('Step')
    axs[1, 0].set_ylabel('Reward')
    
    # Plot 4: Cumulative reward
    cumulative_rewards = np.cumsum(ts.history['rewards'])
    axs[1, 1].plot(cumulative_rewards)
    axs[1, 1].set_title('Cumulative Reward')
    axs[1, 1].set_xlabel('Step')
    axs[1, 1].set_ylabel('Cumulative Reward')
    
    # Add overall title and adjust layout
    plt.suptitle('Thompson Sampling Simulation Results', fontsize=16)
    plt.tight_layout(rect=[0, 0, 1, 0.95])
    plt.show()


# Example usage
if __name__ == "__main__":
    # Set random seed for reproducibility
    np.random.seed(42)
    
    # Define the problem
    n_arms = 5
    true_probabilities = [0.1, 0.2, 0.3, 0.8, 0.5]  # Arm 3 is the best
    n_steps = 1000
    
    # Run simulation
    ts = run_bandit_simulation(n_arms, true_probabilities, n_steps)
    
    # Print results
    print("Simulation completed.")
    print(f"Total reward: {ts.history['cumulative_reward']}")
    print("\nArm statistics:")
    for i in range(n_arms):
        pulls = ts.pulls[i]
        expected_reward = ts.get_expected_rewards()[i]
        print(f"Arm {i}: {pulls} pulls, {expected_reward:.3f} expected reward (true: {true_probabilities[i]:.3f})")
    
    # Plot results
    plot_results(ts, true_probabilities)
```

I've provided two implementations of Thompson Sampling:

1. **thompson_sampling.py**: A comprehensive implementation with support for both Bernoulli (Beta prior) and Gaussian bandits, along with detailed metrics tracking and visualization functions.

2. **thompson_sampling_simplified.py**: A simpler implementation focused specifically on Bernoulli bandits with Beta priors, which might be easier to understand for beginners.

Thompson Sampling works by:
1. Maintaining a probability distribution (the posterior) for each arm's reward
2. At each round, sampling from each distribution and selecting the arm with highest sample
3. After observing the reward, updating the posterior distribution for the selected arm
4. Repeating this process to balance exploration and exploitation

The key advantage of Thompson Sampling is that it naturally balances exploration and exploitation through its probabilistic approach, performing well in practice and having strong theoretical guarantees.
